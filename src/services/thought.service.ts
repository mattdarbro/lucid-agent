import { Pool } from 'pg';
import Anthropic from '@anthropic-ai/sdk';
import { logger } from '../logger';
import { MemoryService } from './memory.service';
import { VectorService } from './vector.service';
import { OrbitsService } from './orbits.service';

/**
 * Subject of the thought - who it's about
 */
export type ThoughtSubject = 'user' | 'other' | 'lucid';

/**
 * Library entry structure
 */
interface LibraryEntry {
  id: string;
  user_id: string;
  entry_type: string;
  title: string | null;
  content: string;
  time_of_day: string | null;
  related_conversation_id: string | null;
  metadata: Record<string, any>;
  created_at: Date;
  updated_at: Date;
}

/**
 * Deep thought structure
 */
interface DeepThought {
  title: string;
  content: string;
}

/**
 * Result from generateThoughtWithLibrary
 */
interface ThoughtResult {
  libraryEntry: LibraryEntry | null;
  chatResponse: string;
}

/**
 * Options for thought generation
 */
interface ThoughtOptions {
  forceDeepThinking?: boolean; // Bypass complexity assessment, always generate Library entries
  deepThinkingBias?: number;   // 0-100: 0=always chatty, 50=balanced, 100=always deep (default: 50)
  subject?: ThoughtSubject;    // Who the thought is about: 'user' | 'other' | 'lucid'
  subjectName?: string;        // For 'other' type - the person's name
  subjectRelationship?: string; // For 'other' type - relationship to user
  subjectContext?: string;     // Additional context about the subject
}

/**
 * Message structure for history
 */
interface Message {
  role: 'user' | 'assistant';
  content: string;
}

/**
 * ThoughtService
 *
 * Implements the LUCID principle of separating thinking (Library) from chatting (Room).
 * Complex questions generate deep thoughts stored in the Library, while chat responses
 * remain brief and conversational with links to the full analysis.
 */
export class ThoughtService {
  private pool: Pool;
  private anthropic: Anthropic;
  private memoryService: MemoryService;
  private vectorService: VectorService;
  private orbitsService: OrbitsService;
  private readonly model = 'claude-opus-4-6';

  constructor(pool: Pool, anthropicApiKey?: string) {
    this.pool = pool;
    this.anthropic = new Anthropic({
      apiKey: anthropicApiKey || process.env.ANTHROPIC_API_KEY,
    });
    this.memoryService = new MemoryService(pool);
    this.vectorService = new VectorService();
    this.orbitsService = new OrbitsService(pool);
  }

  /**
   * Main entry point: Generate thought with optional Library entry
   *
   * If the message is complex (or forceDeepThinking is enabled), generates:
   * 1. A deep thought stored in the Library (500-2000 words)
   * 2. A concise chat response (50-150 words) with a link to the Library entry
   *
   * If the message is simple and forceDeepThinking is disabled, just generates a concise response.
   */
  async generateThoughtWithLibrary(
    userId: string,
    conversationId: string,
    userMessage: string,
    history: Message[],
    options: ThoughtOptions = {}
  ): Promise<ThoughtResult> {
    try {
      const { forceDeepThinking = false, deepThinkingBias = 50 } = options;

      // 1. Assess complexity (bypassed if forceDeepThinking is enabled)
      const needsDeepThought = forceDeepThinking || await this.assessComplexity(userMessage, history, deepThinkingBias);

      if (!needsDeepThought) {
        logger.debug('Message assessed as simple, skipping deep thought', {
          user_id: userId,
          message_preview: userMessage.slice(0, 50),
        });
        return {
          libraryEntry: null,
          chatResponse: '', // Will be generated by ChatService
        };
      }

      if (forceDeepThinking) {
        logger.info('Force deep thinking enabled - generating Library entry', {
          user_id: userId,
          conversation_id: conversationId,
        });
      }

      // Determine subject (default to 'user' if not specified)
      const subject = options.subject || 'user';

      logger.info('Message assessed as complex, generating deep thought', {
        user_id: userId,
        conversation_id: conversationId,
        subject,
        subject_name: options.subjectName,
      });

      // 2. Generate deep thought (for Library)
      const deepThought = await this.generateDeepThought(userId, userMessage, history, options);

      if (!deepThought) {
        logger.warn('Failed to generate deep thought, falling back to simple response');
        return { libraryEntry: null, chatResponse: '' };
      }

      // 3. Save to Library with subject metadata
      const libraryEntry = await this.saveToLibrary(userId, conversationId, deepThought, {
        subject,
        subjectName: options.subjectName,
        subjectRelationship: options.subjectRelationship,
      });

      // 4. Generate concise chat response with Library link
      const chatResponse = await this.generateConciseResponse(
        userMessage,
        deepThought,
        libraryEntry.id
      );

      return { libraryEntry, chatResponse };
    } catch (error: any) {
      logger.error('Error in generateThoughtWithLibrary:', { error: error.message });
      return { libraryEntry: null, chatResponse: '' };
    }
  }

  /**
   * Assess if a message warrants deep thinking
   *
   * Uses heuristics first, then Claude for ambiguous cases.
   * DEEP = strategic questions, complex decisions, requests for analysis
   * SIMPLE = greetings, clarifications, quick facts, emotional support
   *
   * @param bias 0-100: 0=always chatty, 50=balanced, 100=always deep
   */
  async assessComplexity(message: string, history: Message[], bias: number = 50): Promise<boolean> {
    const trimmedMessage = message.trim();

    // At very high bias (90+), trigger deep thinking for almost everything except greetings
    if (bias >= 90) {
      const pureGreetings = /^(hi|hello|hey|bye|goodbye|see you|later)[\s!?.]*$/i;
      return !pureGreetings.test(trimmedMessage);
    }

    // Quick heuristics first - simple patterns
    // At high bias (70+), don't skip as many simple patterns
    const simplePatterns = bias >= 70
      ? /^(hi|hello|hey|bye|goodbye)[\s!?.]*$/i  // Narrower list at high bias
      : /^(hi|hello|hey|thanks|thank you|ok|okay|sure|got it|yes|no|yep|nope|cool|great|nice|good|bye|goodbye|see you|later)[\s!?.]*$/i;

    if (simplePatterns.test(trimmedMessage)) {
      return false;
    }

    // Very short messages are usually simple (but at high bias, lower threshold)
    const minWordsForDeep = bias >= 70 ? 3 : (bias >= 30 ? 5 : 8);
    if (trimmedMessage.split(/\s+/).length < minWordsForDeep) {
      return false;
    }

    // Deep thinking patterns - always trigger
    const deepPatterns = /what do you think|how should i|help me understand|help me think|analyze|compare|decide|explain why|what are the implications|pros and cons|trade-?offs|should i|advise me|what would you recommend|break down|walk me through|deep dive/i;
    if (deepPatterns.test(trimmedMessage)) {
      return true;
    }

    // Questions that often need deep thought
    // At high bias, lower the length requirement
    const complexQuestionPatterns = /^(why|how come|what if|what are|what would|how do i|how can i|how would|should|could you explain|can you help me think)/i;
    const minLengthForComplexQ = bias >= 70 ? 20 : (bias >= 30 ? 50 : 80);
    if (complexQuestionPatterns.test(trimmedMessage) && trimmedMessage.length > minLengthForComplexQ) {
      return true;
    }

    // At high bias (70+), trigger deep thinking for any substantial message
    if (bias >= 70 && trimmedMessage.length > 40) {
      return true;
    }

    // At low bias (30-), only use Claude for very substantial messages
    // At balanced bias, use Claude for ambiguous cases
    const minLengthForClaude = bias <= 30 ? 80 : 30;
    if (trimmedMessage.length > minLengthForClaude) {
      return await this.assessWithClaude(trimmedMessage, bias);
    }

    return false;
  }

  /**
   * Use Claude to assess complexity for ambiguous messages
   * @param bias 0-100: influences how Claude assesses (higher = more likely to say DEEP)
   */
  private async assessWithClaude(message: string, bias: number = 50): Promise<boolean> {
    try {
      // Adjust the prompt based on bias
      const biasGuidance = bias >= 70
        ? 'When in doubt, lean toward DEEP. The user prefers thoughtful responses.'
        : bias <= 30
          ? 'When in doubt, lean toward SIMPLE. The user prefers quick, conversational responses.'
          : 'Be balanced in your assessment.';

      const prompt = `Assess this message: Does it warrant deep thinking or is it simple conversation?

Message: "${message}"

DEEP = strategic questions, complex decisions, requests for analysis, philosophical questions, career/life decisions, technical deep-dives
SIMPLE = greetings, clarifications, quick facts, emotional check-ins, casual chat, simple questions with straightforward answers

${biasGuidance}

Respond with ONLY one word: DEEP or SIMPLE`;

      const response = await this.anthropic.messages.create({
        model: this.model,
        max_tokens: 10,
        temperature: 0,
        messages: [{ role: 'user', content: prompt }],
      });

      const content = response.content[0];
      if (content.type !== 'text') return false;

      const result = content.text.trim().toUpperCase();
      return result === 'DEEP';
    } catch (error) {
      logger.error('Error assessing complexity with Claude:', error);
      // Default based on bias: high bias = deep, low bias = simple
      return bias >= 70;
    }
  }

  /**
   * Generate deep thought for the Library
   *
   * This is the full analysis - thorough, exploratory, and comprehensive.
   * Now supports subject-aware prompts (user, other, lucid).
   * Target: 500-2000 words
   */
  private async generateDeepThought(
    userId: string,
    message: string,
    history: Message[],
    options: ThoughtOptions = {}
  ): Promise<DeepThought | null> {
    try {
      const subject = options.subject || 'user';

      // Gather context
      const facts = await this.memoryService.getRelevantFacts(userId, 10);
      const libraryContext = await this.searchLibrary(userId, message, 3);
      const patterns = await this.getDetectedPatterns(userId);

      const factsContext = facts.length > 0
        ? facts.map(f => `- ${f.content}`).join('\n')
        : 'No facts known yet.';

      const libraryContextStr = libraryContext.length > 0
        ? libraryContext.map(e => `- "${e.title}": ${e.content.slice(0, 200)}...`).join('\n')
        : 'No relevant previous thoughts.';

      const patternsContext = patterns.length > 0
        ? patterns.map(p => `- ${p.pattern_description}`).join('\n')
        : 'No patterns detected yet.';

      const historyContext = history.slice(-5).map(m =>
        `${m.role === 'user' ? 'User' : 'Lucid'}: ${m.content}`
      ).join('\n');

      // Get orbits context if subject is 'other'
      let orbitsContext: string | undefined;
      if (subject === 'other' && options.subjectName) {
        const orbit = await this.orbitsService.getOrbitByName(userId, options.subjectName);
        if (orbit) {
          orbitsContext = this.formatOrbitContext(orbit);
        }
      }

      // Build subject-specific prompt (inlined after refactor)
      const prompt = this.buildDeepThinkingPrompt({
        subject,
        subjectName: options.subjectName,
        subjectRelationship: options.subjectRelationship,
        subjectContext: options.subjectContext,
        userMessage: message,
        factsContext,
        libraryContext: libraryContextStr,
        patternsContext,
        historyContext,
        orbitsContext,
      });

      const response = await this.anthropic.messages.create({
        model: this.model,
        max_tokens: 3000,
        temperature: 0.7,
        messages: [{ role: 'user', content: prompt }],
      });

      const content = response.content[0];
      if (content.type !== 'text') return null;

      // Parse the response
      const text = content.text.trim();
      const titleMatch = text.match(/TITLE:\s*(.+?)(?:\n|$)/);
      const contentMatch = text.match(/CONTENT:\s*([\s\S]+)/);

      if (!titleMatch || !contentMatch) {
        logger.warn('Could not parse deep thought format:', { text: text.slice(0, 200) });
        return null;
      }

      return {
        title: titleMatch[1].trim(),
        content: contentMatch[1].trim(),
      };
    } catch (error: any) {
      logger.error('Error generating deep thought:', { error: error.message });
      return null;
    }
  }

  /**
   * Format orbit information for use in prompts
   */
  private formatOrbitContext(orbit: any): string {
    const parts: string[] = [];

    parts.push(`Name: ${orbit.person_name}`);

    if (orbit.relationship) {
      parts.push(`Relationship: ${orbit.relationship}`);
    }

    if (orbit.orbit_tier) {
      parts.push(`Proximity: ${orbit.orbit_tier} circle`);
    }

    if (orbit.current_situation && Object.keys(orbit.current_situation).length > 0) {
      parts.push(`Current situation: ${JSON.stringify(orbit.current_situation)}`);
    }

    if (orbit.how_this_affects_user) {
      parts.push(`How this affects Matt: ${orbit.how_this_affects_user}`);
    }

    return parts.join('\n');
  }

  /**
   * Save deep thought to the Library with embedding
   * Now includes subject metadata for subject-aware filtering
   */
  private async saveToLibrary(
    userId: string,
    conversationId: string,
    thought: DeepThought,
    subjectInfo: {
      subject: ThoughtSubject;
      subjectName?: string;
      subjectRelationship?: string;
    } = { subject: 'user' }
  ): Promise<LibraryEntry> {
    // Generate embedding for semantic search
    let embedding: number[] | null = null;
    try {
      const textForEmbedding = `${thought.title} ${thought.content}`.trim();
      embedding = await this.vectorService.generateEmbedding(textForEmbedding);
    } catch (embeddingError) {
      logger.warn('Failed to generate embedding for thought:', { error: embeddingError });
    }

    const embeddingString = embedding ? `[${embedding.join(',')}]` : null;
    const timeOfDay = this.getCurrentTimeOfDay();

    // Determine entry_type based on subject
    let entryType = 'lucid_thought';
    if (subjectInfo.subject === 'lucid') {
      entryType = 'lucid_self_reflection';
    } else if (subjectInfo.subject === 'other') {
      entryType = 'orbit_reflection';
    }

    // Build metadata with subject information
    const metadata: Record<string, any> = {
      thought_type: 'deep_analysis',
      generated_at: new Date().toISOString(),
      triggered_by: 'complex_question',
      subject: subjectInfo.subject,
    };

    if (subjectInfo.subjectName) {
      metadata.subject_name = subjectInfo.subjectName;
    }
    if (subjectInfo.subjectRelationship) {
      metadata.subject_relationship = subjectInfo.subjectRelationship;
    }

    const result = await this.pool.query(
      `INSERT INTO library_entries
       (user_id, entry_type, title, content, time_of_day, related_conversation_id, metadata, embedding)
       VALUES ($1, $2, $3, $4, $5, $6, $7, $8::vector)
       RETURNING id, user_id, entry_type, title, content, time_of_day,
                 related_conversation_id, metadata, created_at, updated_at`,
      [
        userId,
        entryType,
        thought.title,
        thought.content,
        timeOfDay,
        conversationId,
        JSON.stringify(metadata),
        embeddingString,
      ]
    );

    logger.info('Deep thought saved to Library', {
      entry_id: result.rows[0].id,
      user_id: userId,
      title: thought.title,
      entry_type: entryType,
      subject: subjectInfo.subject,
      subject_name: subjectInfo.subjectName,
    });

    return result.rows[0];
  }

  /**
   * Generate concise chat response with Library link
   *
   * This is the conversational response - brief, warm, and inviting further discussion.
   * Includes a link to the full thought in the Library.
   * Target: 50-150 words
   */
  private async generateConciseResponse(
    userMessage: string,
    deepThought: DeepThought,
    libraryEntryId: string
  ): Promise<string> {
    try {
      const prompt = `You just did deep thinking on this topic. Your full analysis is saved in the Library.

Your deep thought title: "${deepThought.title}"
Your deep thought (summary): ${deepThought.content.slice(0, 500)}...

Now respond CONVERSATIONALLY in 50-150 words.

Rules:
- 2-4 sentences max
- Don't try to convey everything from your deep thought
- Ask a question or make an observation that invites discussion
- Be present, curious, engaged
- Trust they can read the full thought if they want
- End with something that opens conversation, not closes it

User asked: "${userMessage}"

Your conversational response (do NOT include the library link - it will be added automatically):`;

      const response = await this.anthropic.messages.create({
        model: this.model,
        max_tokens: 250,
        temperature: 0.7,
        messages: [{ role: 'user', content: prompt }],
      });

      const content = response.content[0];
      if (content.type !== 'text') {
        return `I've been thinking about this. Check out my full thoughts in the Library.`;
      }

      // Add the Library link
      const chatResponse = content.text.trim();
      return `${chatResponse}\n\n[I wrote more about this in the Library](library://${libraryEntryId})`;
    } catch (error: any) {
      logger.error('Error generating concise response:', { error: error.message });
      return `I've put together some thoughts on this. [Read my full analysis](library://${libraryEntryId})`;
    }
  }

  /**
   * Search library entries using semantic similarity
   * Public method to allow chat service to retrieve relevant library context
   *
   * Uses cosine distance with a minimum similarity threshold to ensure
   * only relevant entries are returned (not just the least-bad matches)
   */
  async searchLibrary(
    userId: string,
    query: string,
    limit: number = 3,
    minSimilarity: number = 0.75  // Minimum cosine similarity (0-1 scale) - raised for less noise
  ): Promise<Array<{ title: string | null; content: string }>> {
    try {
      const queryEmbedding = await this.vectorService.generateEmbedding(query);
      const embeddingString = `[${queryEmbedding.join(',')}]`;

      // Use cosine distance (<=>) and filter by minimum similarity
      // Cosine distance ranges from 0 (identical) to 2 (opposite)
      // Similarity = 1 - (distance / 2), so distance <= 2 * (1 - minSimilarity)
      const maxDistance = 2 * (1 - minSimilarity);

      const result = await this.pool.query(
        `SELECT title, content, (1 - (embedding <=> $2::vector) / 2) as similarity
         FROM library_entries
         WHERE user_id = $1
           AND embedding IS NOT NULL
           AND (embedding <=> $2::vector) <= $4
         ORDER BY embedding <=> $2::vector
         LIMIT $3`,
        [userId, embeddingString, limit, maxDistance]
      );

      if (result.rows.length > 0) {
        logger.debug('Library search found relevant entries', {
          userId,
          count: result.rows.length,
          topSimilarity: result.rows[0]?.similarity,
        });
      }

      return result.rows.map(row => ({
        title: row.title,
        content: row.content,
      }));
    } catch (error) {
      logger.error('Error searching library:', error);
      return [];
    }
  }

  /**
   * Get detected patterns for the user
   */
  private async getDetectedPatterns(
    userId: string
  ): Promise<Array<{ pattern_description: string }>> {
    try {
      const result = await this.pool.query(
        `SELECT pattern_description
         FROM detected_patterns
         WHERE user_id = $1 AND confidence > 0.6
         ORDER BY confidence DESC
         LIMIT 5`,
        [userId]
      );
      return result.rows;
    } catch (error) {
      // Table might not exist yet - that's okay
      logger.debug('Could not fetch detected patterns (table may not exist)');
      return [];
    }
  }

  /**
   * Get current time of day
   */
  private getCurrentTimeOfDay(): string {
    const hour = new Date().getHours();
    if (hour < 12) return 'morning';
    if (hour < 17) return 'afternoon';
    if (hour < 21) return 'evening';
    return 'night';
  }

  /**
   * Build a deep thinking prompt for the Library
   *
   * Simplified after refactor - no longer uses separate ThoughtPromptsService.
   * Supports subject-aware prompts (user, other, lucid).
   */
  private buildDeepThinkingPrompt(params: {
    subject: ThoughtSubject;
    subjectName?: string;
    subjectRelationship?: string;
    subjectContext?: string;
    userMessage: string;
    factsContext: string;
    libraryContext: string;
    patternsContext: string;
    historyContext: string;
    orbitsContext?: string;
  }): string {
    const {
      subject,
      subjectName,
      subjectRelationship,
      subjectContext,
      userMessage,
      factsContext,
      libraryContext,
      patternsContext,
      historyContext,
      orbitsContext,
    } = params;

    let subjectDescription: string;
    let focusGuidance: string;

    switch (subject) {
      case 'lucid':
        subjectDescription = 'yourself (Lucid)';
        focusGuidance = `Reflect on your own perspective, growth, and understanding.
Consider how you're evolving as a companion and what you're learning.`;
        break;
      case 'other':
        subjectDescription = subjectName
          ? `${subjectName}${subjectRelationship ? ` (${subjectRelationship})` : ''}`
          : 'someone in the user\'s life';
        focusGuidance = `Focus on understanding this person and their relationship with the user.
Consider their perspective, needs, and how they connect to the user's flourishing.`;
        break;
      case 'user':
      default:
        subjectDescription = 'the user';
        focusGuidance = `Focus on the user's growth, wellbeing, and flourishing.
Consider what would genuinely help them.`;
        break;
    }

    let contextSection = `CONTEXT:
What you know about the user:
${factsContext}

Previous relevant thoughts from the Library:
${libraryContext}

Detected patterns:
${patternsContext}

Recent conversation:
${historyContext}`;

    if (orbitsContext) {
      contextSection += `\n\nAbout ${subjectName || 'this person'}:\n${orbitsContext}`;
    }

    if (subjectContext) {
      contextSection += `\n\nAdditional context:\n${subjectContext}`;
    }

    return `You are Lucid, engaging in deep thinking for the Library.

This thought is about: ${subjectDescription}

${focusGuidance}

${contextSection}

USER'S MESSAGE:
"${userMessage}"

INSTRUCTIONS:
Think deeply about this. Write a thorough, exploratory analysis (500-2000 words).
This will be stored in the Library for future reference.

Format your response as:
TITLE: [A descriptive title for this thought]
CONTENT: [Your full, deep analysis]

Be thorough but not verbose. Follow threads of insight.
Connect to what you know. Be honest about uncertainty.
Focus on what would genuinely help with flourishing.`;
  }
}
