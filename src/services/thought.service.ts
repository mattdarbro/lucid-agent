import { Pool } from 'pg';
import Anthropic from '@anthropic-ai/sdk';
import { logger } from '../logger';
import { MemoryService } from './memory.service';
import { VectorService } from './vector.service';

/**
 * Library entry structure
 */
interface LibraryEntry {
  id: string;
  user_id: string;
  entry_type: string;
  title: string | null;
  content: string;
  time_of_day: string | null;
  related_conversation_id: string | null;
  metadata: Record<string, any>;
  created_at: Date;
  updated_at: Date;
}

/**
 * Deep thought structure
 */
interface DeepThought {
  title: string;
  content: string;
}

/**
 * Result from generateThoughtWithLibrary
 */
interface ThoughtResult {
  libraryEntry: LibraryEntry | null;
  chatResponse: string;
}

/**
 * Options for thought generation
 */
interface ThoughtOptions {
  forceDeepThinking?: boolean; // Bypass complexity assessment, always generate Library entries
}

/**
 * Message structure for history
 */
interface Message {
  role: 'user' | 'assistant';
  content: string;
}

/**
 * ThoughtService
 *
 * Implements the LUCID principle of separating thinking (Library) from chatting (Room).
 * Complex questions generate deep thoughts stored in the Library, while chat responses
 * remain brief and conversational with links to the full analysis.
 */
export class ThoughtService {
  private pool: Pool;
  private anthropic: Anthropic;
  private memoryService: MemoryService;
  private vectorService: VectorService;
  private readonly model = 'claude-opus-4-5-20251101';

  constructor(pool: Pool, anthropicApiKey?: string) {
    this.pool = pool;
    this.anthropic = new Anthropic({
      apiKey: anthropicApiKey || process.env.ANTHROPIC_API_KEY,
    });
    this.memoryService = new MemoryService(pool);
    this.vectorService = new VectorService();
  }

  /**
   * Main entry point: Generate thought with optional Library entry
   *
   * If the message is complex (or forceDeepThinking is enabled), generates:
   * 1. A deep thought stored in the Library (500-2000 words)
   * 2. A concise chat response (50-150 words) with a link to the Library entry
   *
   * If the message is simple and forceDeepThinking is disabled, just generates a concise response.
   */
  async generateThoughtWithLibrary(
    userId: string,
    conversationId: string,
    userMessage: string,
    history: Message[],
    options: ThoughtOptions = {}
  ): Promise<ThoughtResult> {
    try {
      const { forceDeepThinking = false } = options;

      // 1. Assess complexity (bypassed if forceDeepThinking is enabled)
      const needsDeepThought = forceDeepThinking || await this.assessComplexity(userMessage, history);

      if (!needsDeepThought) {
        logger.debug('Message assessed as simple, skipping deep thought', {
          user_id: userId,
          message_preview: userMessage.slice(0, 50),
        });
        return {
          libraryEntry: null,
          chatResponse: '', // Will be generated by ChatService
        };
      }

      if (forceDeepThinking) {
        logger.info('Force deep thinking enabled - generating Library entry', {
          user_id: userId,
          conversation_id: conversationId,
        });
      }

      logger.info('Message assessed as complex, generating deep thought', {
        user_id: userId,
        conversation_id: conversationId,
      });

      // 2. Generate deep thought (for Library)
      const deepThought = await this.generateDeepThought(userId, userMessage, history);

      if (!deepThought) {
        logger.warn('Failed to generate deep thought, falling back to simple response');
        return { libraryEntry: null, chatResponse: '' };
      }

      // 3. Save to Library
      const libraryEntry = await this.saveToLibrary(userId, conversationId, deepThought);

      // 4. Generate concise chat response with Library link
      const chatResponse = await this.generateConciseResponse(
        userMessage,
        deepThought,
        libraryEntry.id
      );

      return { libraryEntry, chatResponse };
    } catch (error: any) {
      logger.error('Error in generateThoughtWithLibrary:', { error: error.message });
      return { libraryEntry: null, chatResponse: '' };
    }
  }

  /**
   * Assess if a message warrants deep thinking
   *
   * Uses heuristics first, then Claude for ambiguous cases.
   * DEEP = strategic questions, complex decisions, requests for analysis
   * SIMPLE = greetings, clarifications, quick facts, emotional support
   */
  async assessComplexity(message: string, history: Message[]): Promise<boolean> {
    const trimmedMessage = message.trim();

    // Quick heuristics first - simple patterns
    const simplePatterns = /^(hi|hello|hey|thanks|thank you|ok|okay|sure|got it|yes|no|yep|nope|cool|great|nice|good|bye|goodbye|see you|later)[\s!?.]*$/i;
    if (simplePatterns.test(trimmedMessage)) {
      return false;
    }

    // Very short messages are usually simple
    if (trimmedMessage.split(/\s+/).length < 5) {
      return false;
    }

    // Deep thinking patterns
    const deepPatterns = /what do you think|how should i|help me understand|help me think|analyze|compare|decide|explain why|what are the implications|pros and cons|trade-?offs|should i|advise me|what would you recommend|break down|walk me through|deep dive/i;
    if (deepPatterns.test(trimmedMessage)) {
      return true;
    }

    // Questions that often need deep thought
    const complexQuestionPatterns = /^(why|how come|what if|what are|what would|how do i|how can i|how would|should|could you explain|can you help me think)/i;
    if (complexQuestionPatterns.test(trimmedMessage) && trimmedMessage.length > 50) {
      return true;
    }

    // For ambiguous cases, use Claude to assess
    if (trimmedMessage.length > 30) {
      return await this.assessWithClaude(trimmedMessage);
    }

    return false;
  }

  /**
   * Use Claude to assess complexity for ambiguous messages
   */
  private async assessWithClaude(message: string): Promise<boolean> {
    try {
      const prompt = `Assess this message: Does it warrant deep thinking or is it simple conversation?

Message: "${message}"

DEEP = strategic questions, complex decisions, requests for analysis, philosophical questions, career/life decisions, technical deep-dives
SIMPLE = greetings, clarifications, quick facts, emotional check-ins, casual chat, simple questions with straightforward answers

Respond with ONLY one word: DEEP or SIMPLE`;

      const response = await this.anthropic.messages.create({
        model: this.model,
        max_tokens: 10,
        temperature: 0,
        messages: [{ role: 'user', content: prompt }],
      });

      const content = response.content[0];
      if (content.type !== 'text') return false;

      const result = content.text.trim().toUpperCase();
      return result === 'DEEP';
    } catch (error) {
      logger.error('Error assessing complexity with Claude:', error);
      return false; // Default to simple on error
    }
  }

  /**
   * Generate deep thought for the Library
   *
   * This is the full analysis - thorough, exploratory, and comprehensive.
   * Target: 500-2000 words
   */
  private async generateDeepThought(
    userId: string,
    message: string,
    history: Message[]
  ): Promise<DeepThought | null> {
    try {
      // Gather context
      const facts = await this.memoryService.getRelevantFacts(userId, 10);
      const libraryContext = await this.searchLibrary(userId, message, 3);
      const patterns = await this.getDetectedPatterns(userId);

      const factsContext = facts.length > 0
        ? facts.map(f => `- ${f.content}`).join('\n')
        : 'No facts known yet.';

      const libraryContextStr = libraryContext.length > 0
        ? libraryContext.map(e => `- "${e.title}": ${e.content.slice(0, 200)}...`).join('\n')
        : 'No relevant previous thoughts.';

      const patternsContext = patterns.length > 0
        ? patterns.map(p => `- ${p.pattern_description}`).join('\n')
        : 'No patterns detected yet.';

      const historyContext = history.slice(-5).map(m =>
        `${m.role === 'user' ? 'User' : 'Lucid'}: ${m.content}`
      ).join('\n');

      const prompt = `Think deeply about this. Take your time. Explore fully.

User's message: "${message}"

What you know about them:
${factsContext}

Relevant previous thoughts from your Library:
${libraryContextStr}

Patterns you've detected:
${patternsContext}

Recent conversation:
${historyContext}

Write your COMPLETE thought process. This is for the Library, not chat.
- Explore multiple angles
- Consider implications and trade-offs
- Note uncertainties and assumptions
- Make connections to what you know about them
- Be thorough but focused (500-2000 words)
- Write as yourself (Lucid), thinking through this WITH them

Format your response EXACTLY as:
TITLE: [A descriptive title for this thought - 3-10 words]
CONTENT: [Your full analysis]

Do not include any other text outside this format.`;

      const response = await this.anthropic.messages.create({
        model: this.model,
        max_tokens: 3000,
        temperature: 0.7,
        messages: [{ role: 'user', content: prompt }],
      });

      const content = response.content[0];
      if (content.type !== 'text') return null;

      // Parse the response
      const text = content.text.trim();
      const titleMatch = text.match(/TITLE:\s*(.+?)(?:\n|$)/);
      const contentMatch = text.match(/CONTENT:\s*([\s\S]+)/);

      if (!titleMatch || !contentMatch) {
        logger.warn('Could not parse deep thought format:', { text: text.slice(0, 200) });
        return null;
      }

      return {
        title: titleMatch[1].trim(),
        content: contentMatch[1].trim(),
      };
    } catch (error: any) {
      logger.error('Error generating deep thought:', { error: error.message });
      return null;
    }
  }

  /**
   * Save deep thought to the Library with embedding
   */
  private async saveToLibrary(
    userId: string,
    conversationId: string,
    thought: DeepThought
  ): Promise<LibraryEntry> {
    // Generate embedding for semantic search
    let embedding: number[] | null = null;
    try {
      const textForEmbedding = `${thought.title} ${thought.content}`.trim();
      embedding = await this.vectorService.generateEmbedding(textForEmbedding);
    } catch (embeddingError) {
      logger.warn('Failed to generate embedding for thought:', { error: embeddingError });
    }

    const embeddingString = embedding ? `[${embedding.join(',')}]` : null;
    const timeOfDay = this.getCurrentTimeOfDay();

    const result = await this.pool.query(
      `INSERT INTO library_entries
       (user_id, entry_type, title, content, time_of_day, related_conversation_id, metadata, embedding)
       VALUES ($1, 'lucid_thought', $2, $3, $4, $5, $6, $7::vector)
       RETURNING id, user_id, entry_type, title, content, time_of_day,
                 related_conversation_id, metadata, created_at, updated_at`,
      [
        userId,
        thought.title,
        thought.content,
        timeOfDay,
        conversationId,
        JSON.stringify({
          thought_type: 'deep_analysis',
          generated_at: new Date().toISOString(),
          triggered_by: 'complex_question',
        }),
        embeddingString,
      ]
    );

    logger.info('Deep thought saved to Library', {
      entry_id: result.rows[0].id,
      user_id: userId,
      title: thought.title,
    });

    return result.rows[0];
  }

  /**
   * Generate concise chat response with Library link
   *
   * This is the conversational response - brief, warm, and inviting further discussion.
   * Includes a link to the full thought in the Library.
   * Target: 50-150 words
   */
  private async generateConciseResponse(
    userMessage: string,
    deepThought: DeepThought,
    libraryEntryId: string
  ): Promise<string> {
    try {
      const prompt = `You just did deep thinking on this topic. Your full analysis is saved in the Library.

Your deep thought title: "${deepThought.title}"
Your deep thought (summary): ${deepThought.content.slice(0, 500)}...

Now respond CONVERSATIONALLY in 50-150 words.

Rules:
- 2-4 sentences max
- Don't try to convey everything from your deep thought
- Ask a question or make an observation that invites discussion
- Be present, curious, engaged
- Trust they can read the full thought if they want
- End with something that opens conversation, not closes it

User asked: "${userMessage}"

Your conversational response (do NOT include the library link - it will be added automatically):`;

      const response = await this.anthropic.messages.create({
        model: this.model,
        max_tokens: 250,
        temperature: 0.7,
        messages: [{ role: 'user', content: prompt }],
      });

      const content = response.content[0];
      if (content.type !== 'text') {
        return `I've been thinking about this. Check out my full thoughts in the Library.`;
      }

      // Add the Library link
      const chatResponse = content.text.trim();
      return `${chatResponse}\n\n[I wrote more about this in the Library](library://${libraryEntryId})`;
    } catch (error: any) {
      logger.error('Error generating concise response:', { error: error.message });
      return `I've put together some thoughts on this. [Read my full analysis](library://${libraryEntryId})`;
    }
  }

  /**
   * Search library entries using semantic similarity
   * Public method to allow chat service to retrieve relevant library context
   *
   * Uses cosine distance with a minimum similarity threshold to ensure
   * only relevant entries are returned (not just the least-bad matches)
   */
  async searchLibrary(
    userId: string,
    query: string,
    limit: number = 3,
    minSimilarity: number = 0.3  // Minimum cosine similarity (0-1 scale)
  ): Promise<Array<{ title: string | null; content: string }>> {
    try {
      const queryEmbedding = await this.vectorService.generateEmbedding(query);
      const embeddingString = `[${queryEmbedding.join(',')}]`;

      // Use cosine distance (<=>) and filter by minimum similarity
      // Cosine distance ranges from 0 (identical) to 2 (opposite)
      // Similarity = 1 - (distance / 2), so distance <= 2 * (1 - minSimilarity)
      const maxDistance = 2 * (1 - minSimilarity);

      const result = await this.pool.query(
        `SELECT title, content, (1 - (embedding <=> $2::vector) / 2) as similarity
         FROM library_entries
         WHERE user_id = $1
           AND embedding IS NOT NULL
           AND (embedding <=> $2::vector) <= $4
         ORDER BY embedding <=> $2::vector
         LIMIT $3`,
        [userId, embeddingString, limit, maxDistance]
      );

      if (result.rows.length > 0) {
        logger.debug('Library search found relevant entries', {
          userId,
          count: result.rows.length,
          topSimilarity: result.rows[0]?.similarity,
        });
      }

      return result.rows.map(row => ({
        title: row.title,
        content: row.content,
      }));
    } catch (error) {
      logger.error('Error searching library:', error);
      return [];
    }
  }

  /**
   * Get detected patterns for the user
   */
  private async getDetectedPatterns(
    userId: string
  ): Promise<Array<{ pattern_description: string }>> {
    try {
      const result = await this.pool.query(
        `SELECT pattern_description
         FROM detected_patterns
         WHERE user_id = $1 AND confidence > 0.6
         ORDER BY confidence DESC
         LIMIT 5`,
        [userId]
      );
      return result.rows;
    } catch (error) {
      // Table might not exist yet - that's okay
      logger.debug('Could not fetch detected patterns (table may not exist)');
      return [];
    }
  }

  /**
   * Get current time of day
   */
  private getCurrentTimeOfDay(): string {
    const hour = new Date().getHours();
    if (hour < 12) return 'morning';
    if (hour < 17) return 'afternoon';
    if (hour < 21) return 'evening';
    return 'night';
  }
}
